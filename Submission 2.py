# -*- coding: utf-8 -*-
"""Untitled67.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HXFjDA0TbOn0yIu-9vQYUi4UYqYxGkQH
"""

#import librari pandas_datareader untuk data eksternal dari web
import pandas_datareader as web
 
#import modul datetime merupakan internal datetime
import datetime
 
#datetime.datetime adalah tipe data dalam modul datetime
start = datetime.datetime(1980, 4, 1)
end = datetime.datetime(2020, 9, 20)
 
#metode DataReader musti spesifik namanya
df = web.DataReader("hmc", 'yahoo', start, end)
 
#dataframe disimpan sebagai file csv 
df.to_csv('HMC.csv')

#menampilkan informasi baris dan kolom, serta baris 5 pertama
df=pd.read_csv("HMC.csv")
print('Number of rows and columns:', df.shape)
df.head()

df.info()

df.isnull().sum()

df.columns

df = df[['Adj Close']]
df.tail()

# Normalisasi data
from sklearn import preprocessing

values = df.values.reshape(-1,1)
values = values.astype('float32')
scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)

values

scaled

#membagi data Validation set sebesar 20% dari total dataset
training_size = int(len(scaled) * 0.7)
validasi_size = int(len(scaled) * 0.2)
test_size = int(len(scaled) * 0.1)
training, validasi, test = scaled[0:training_size,:], scaled[training_size:-(test_size),:], scaled[-(test_size):,:]
print(f'training: {len(training)}')
print(f'validation: {len(validasi)}')
print(f'test: {len(test)}')

from itertools import chain

training = np.array(list(chain.from_iterable(training)))
validasi = np.array(list(chain.from_iterable(validasi)))
test = np.array(list(chain.from_iterable(test)))

training

#membagi data sequence menjadi sample
def split_sequence(sequence, n_steps):
	X, Y = list(), list()
	for i in range(len(sequence)):
		end_iX = i + n_steps
		if end_iX > len(sequence)-1:
			break
		seq_X, seq_Y = sequence[i:end_iX], sequence[end_iX]
		X.append(seq_X)
		Y.append(seq_Y)
	return np.array(X), np.array(Y)

n_steps = 12
train_X, train_Y = split_sequence(train, n_steps)
val_X, val_Y = split_sequence(val, n_steps)
test_X, test_Y = split_sequence(test, n_steps)

train_X

n_features = 1
train_X = train_X.reshape((train_X.shape[0], train_X.shape[1], n_features))
val_X = val_X.reshape((val_X.shape[0], val_X.shape[1], n_features))
test_X = test_X.reshape((test_X.shape[0], test_X.shape[1], n_features))

#Membuat arsitektur LSTM
import tensorflow as tf
from keras.layers import LSTM
#Lapisan pertama adalah lapisan LSTM dengan 60 unit.
#fungsi aktivasi relu
#nilai prediksi tunggal yaitu 1
#menggunakan model sequential
model = tf.keras.models.Sequential([
    tf.keras.layers.Bidirectional(LSTM(60), input_shape=(n_steps, n_features)),
    tf.keras.layers.Dense(60, activation='relu'),
    tf.keras.layers.Dense(80, activation='relu'),
    tf.keras.layers.Dense(1)
])

#menggunakan Learning Rate pada Optimizer
optimizer = tf.keras.optimizers.Adam(lr=1.0e-03)
model.compile(optimizer=optimizer,loss=tf.keras.losses.Huber(),metrics=['mae'])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<0.015 and logs.get('val_mae')<0.015):
      print("\nTraining sudah mencapai MAE < 10% skala data !!")
      self.model.stop_training = True

callbacks = myCallback()
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,
            patience=5, min_lr=1.0e-04)

#melatih model
history = model.fit(train_X, train_Y, 
                    epochs=100, batch_size=128, 
                    validation_data=(val_X, val_Y), 
                    verbose=2, callbacks=[callbacks,reduce_lr])

#plot loss dan akurasi pada saat training dan validation
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Training and Validation loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Training', 'Validation'])
plt.show

#memprediksi model text_X
y_hat = model.predict(test_X)
plt.figure(figsize=(10,5))
plt.plot(test_Y, label='true')
plt.plot(y_hat, label='predict')
plt.title('Test Prediction')
plt.legend()
plt.show()

#Balikkan skala nilai prediksi ke nilai semula
y_hat_inverse = scaler.inverse_transform(y_hat.reshape(-1, 1))
test_Y_inverse = scaler.inverse_transform(test_Y.reshape(-1, 1))

#Menghitung skala data
min_value = df.values.min()
max_value = df.values.max()
data_scale = max_value - min_value
ramalan = y_hat_inverse
aktual = test_Y_inverse
errors = ramalan - aktual
mae = np.abs(errors).mean()
print(f'MAE = {mae:.2f} < 10% skala data = {(0.1 * data_scale):.2f}')